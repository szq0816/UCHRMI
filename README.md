# UCMFH
When CLIP Meets Cross-modal Hashing Retrieval: A New Strong Baseline


### Datasets
We release the three experimental datasets as follows:
- MIRFlickr, [[Baidu Pan](https://pan.baidu.com/s/1zv72jFR-L91vOOK5MUUoOw?pwd=1a30)]
- NUS-WIDE, [[Baidu Pan](https://pan.baidu.com/s/1v9C2M_jL593PKtaRFkPyZw?pwd=8qwm)]
- MS COCO, [[Baidu Pan](https://pan.baidu.com/s/14nq1mYz7-75O0aDsuOhExA?pwd=537l)]

### Demo 
Taking MIR Flickr as an example, our model can be trained and verified by the following command:
```bash
bash test-flickr.sh
```

### Citation
If you use this code, please cite it:
```
@article{xia2023clip,
  title={When CLIP meets cross-modal hashing retrieval: A new strong baseline},
  author={Xia, Xinyu and Dong, Guohua and Li, Fengling and Zhu, Lei and Ying, Xiaomin},
  journal={Information Fusion},
  pages={101968},
  year={2023},
  publisher={Elsevier}
}
```